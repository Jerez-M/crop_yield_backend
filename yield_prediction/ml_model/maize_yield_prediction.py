# -*- coding: utf-8 -*-
"""maize_yield_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QMUJDLd7mYd_bTHD7lQDfKh7M58FrC3J
"""

# Imports
import numpy as np
import pandas as pd

import sklearn
import seaborn as sns
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

rainfall_url = ('/content/drive/MyDrive/datasets/yield_prediction/rainfall.csv')
pesticides_url = ('/content/drive/MyDrive/datasets/yield_prediction/pesticides.csv')
temperature_url = ('/content/drive/MyDrive/datasets/yield_prediction/temp.csv')
yield_url = ('/content/drive/MyDrive/datasets/yield_prediction/yield.csv')

# Load data
pesticides_data = pd.read_csv(pesticides_url, sep=',')
rainfall_data = pd.read_csv(rainfall_url, sep=', ')
temperature_data = pd.read_csv(temperature_url, sep=', ')
yield_data = pd.read_csv(yield_url, sep=',')



"""# Data PrepaPreparation"""

rainfall_data
print(rainfall_data)
rainfall_data.info()

rainfall_data.describe()

# Number of countries
len(rainfall_data['Country'].unique())

# Number of years (1901 - 2016)
len(rainfall_data['Year'].unique())

# One year from Afghanistan
rainfall_data.head(12)

# Rain data in years for each country
rainfall_df = rainfall_data.groupby(['Year', 'Country', 'ISO3'], as_index=False, axis=0).sum()
rainfall_df.tail()

temperature_data.tail()

temperature_data.info()

temperature_data.describe()

# Number of countries
len(temperature_data['Country'].unique())

# Number of years (1901 - 2016)
len(temperature_data['Year'].unique())

# One year from Afghanistan
temperature_data.head(12)

# Average temperature data in years for each country
temperature_df =temperature_data.groupby(['Year', 'Country', 'ISO3'], as_index=False, axis=0).sum()
temperature_df

temperature_df.shape

## Yield data

yield_data

yield_data.info()

yield_data.describe()

# Number of countries
len(yield_data['Area'].unique())

# Number of years (1961 - 2019)
len(yield_data['Year'].unique())

# Remove unnecessary columns
yield_df = yield_data.drop(['Domain', 'Element'], axis=1)
yield_df

## Pesticides data

pesticides_data

pesticides_data.info()

pesticides_data.describe()

# Number of countries
len(pesticides_data['Area'].unique())

# Number of years (1990 - 2018)
len(pesticides_data['Year'].unique())

# Remove unnecessary columns
pesticides_df = pesticides_data.drop(['Domain', 'Element'], axis=1)
pesticides_df

"""# Merging and Exploration"""

rainfall_df.rename({'Rainfall - (MM)': 'Rainfall (mm)'}, axis=1, inplace=True)
rainfall_df

temperature_df.rename({'Temperature - (Celsius)': 'Temperature (Celsius)'}, axis=1, inplace=True)
temperature_df

yield_df.rename({'Area': 'Country', 'Value': 'Yield (hg/ha)'}, axis=1, inplace=True)
# yield_df.drop('Unit', axis=1, inplace=True)
yield_df

pesticides_df.rename({'Area': 'Country', 'Value': 'Pesticides (tonnes)'}, axis=1, inplace=True)
pesticides_df.drop(['Unit', 'Item'], axis=1, inplace=True)
pesticides_df

rain_temp_df = pd.merge(rainfall_df, temperature_df, on=['Country', 'Year', 'ISO3'])
rain_temp_yield_df = pd.merge(rain_temp_df, yield_df, on=['Country', 'Year'])
rain_temp_yield_pest_df = pd.merge(rain_temp_yield_df, pesticides_df, on=['Country', 'Year'])
rain_temp_yield_pest_df.drop('ISO3', axis=1, inplace=True)
data = rain_temp_yield_pest_df[['Year', 'Country', 'Item', 'Rainfall (mm)', 'Temperature (Celsius)', 'Pesticides (tonnes)', 'Yield (hg/ha)']]
data

data.info()

data.describe()

# Number of countries
len(data.Country.unique())

# Number of years (1990 - 2016)
len(data.Year.unique())

data.info()

"""# Modelling, Training and Testing"""

from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, max_error
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.model_selection import cross_validate, cross_val_predict
from sklearn.linear_model import LinearRegression, SGDRegressor
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
import time
from scipy.stats import linregress

label_encoder = LabelEncoder()

# Encode the categorical columns in the original data
data.loc[:, 'Country'] = label_encoder.fit_transform(data['Country'])
data.loc[:, 'Item'] = label_encoder.fit_transform(data['Item'])

yy = data['Yield (hg/ha)']
XX = data.drop('Yield (hg/ha)', axis=1)

XX_train, XX_test, yy_train, yy_test = train_test_split(XX, yy, test_size=0.2, random_state=42)

def mean_absolute_percentage_error(yy_true, yy_pred):
    yy_true, yy_pred = np.array(yy_true), np.array(yy_pred)
    return np.mean(np.abs((yy_true - yy_pred) / yy_true)) * 100

def perform_prediction(year, country, item, rainfall, temperature, pesticides):
    # Encode categorical features
    encoded_country = label_encoder.transform([country])[0]
    encoded_item = label_encoder.transform([item])[0]

    # Create the input array
    X = np.array([[year, encoded_country, encoded_item, rainfall, temperature, pesticides]])

    # Perform prediction
    yield_prediction = random_forest_model.predict(X)*30

    return yield_prediction

def plot_regression_results(ax, yy_test, yy_pred, title, estimated_time, scores):

  # linear least-squares
  slope, intercept, rvalue, pvalue, stderr = linregress(yy_test, yy_pred)
  ax.plot([yy_test.min(), yy_test.max()], [intercept+yy_test.min()*slope, intercept+yy_test.max()*slope], '--r')

  ax.scatter(yy_test, yy_pred, alpha=0.7)

  # Display the values ​​in a box
  extra = plt.Rectangle((0, 0), 0, 0, fc="w", fill=False,
                          edgecolor='none', linewidth=0)
  ax.legend([extra], [scores], loc='upper left')

  ax.set_xlabel('Actual values in tonnes')
  ax.set_ylabel('Predictes values in tonnes')
  ax.set_title('{}\nTrained in {:.2f} Milliseconds'.format(name, estimated_time*1000))

regression_model = LinearRegression()
dtr = DecisionTreeRegressor()
gbr = GradientBoostingRegressor()
knn = KNeighborsRegressor(n_neighbors=5)
random_forest_model = RandomForestRegressor()

start_time = time.time()
random_forest_model.fit(XX_train, yy_train)
yy_pred = random_forest_model.predict(XX_test)

estimated_time = time.time() - start_time
print(yy_pred, "estimated_time is: ", estimated_time)

yield_prediction = perform_prediction(2024, 13, 13, 820, 20, 2230)
print("Yield Prediction:", yield_prediction)

# multiply produced yield by 30 which is the estimate total hectres of land covered in Zimbabwe

from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, max_error
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.model_selection import cross_validate, cross_val_predict
from sklearn.linear_model import LinearRegression, SGDRegressor
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeRegressor

import time
from scipy.stats import linregress

lin = LinearRegression()
dtr = DecisionTreeRegressor()
gbr = GradientBoostingRegressor()
knn = KNeighborsRegressor(n_neighbors=5)
rfr = RandomForestRegressor()

estimators = [('Linear Regression', lin),
              ('Decision Tree Regression', dtr),
              ('Gradient Boosting Regression', gbr),
              ('K-nearest Neighbour 5', knn),
              ('Random Forest Regression', rfr)]

fig, axs = plt.subplots(nrows=2, ncols=3, sharex=True, sharey=True, figsize=(20, 13))
axs = np.ravel(axs)

for ax, (name, est) in zip(axs, estimators):
  start_time = time.time()
  # Training the model
  est.fit(XX_train, yy_train)
  y_pred = est.predict(XX_test)
  estimated_time = time.time() - start_time
  print(y_pred)

  # View the model
  plot_regression_results(ax, yy_test, yy_pred, name, estimated_time,
                          (r'$R^2$ = {:.2f}' + '\n' +
                          r'MAE = {:.0f}' + '\n' +
                          r'MSE = {:.0f}' + '\n' +
                          r'RMSE = {:.0f}' + '\n' +
                          r'MAX = {:.0f}' + '\n' +
                          r'MAPE = {:.2f}%')
                          .format(r2_score(yy_test, yy_pred),
                                  mean_absolute_error(yy_test, yy_pred),
                                  mean_squared_error(yy_test, yy_pred),
                                  mean_squared_error(yy_test, yy_pred, squared=False),
                                  max_error(yy_test, yy_pred),
                                  mean_absolute_percentage_error(yy_test, yy_pred)))

plt.suptitle('Regression results')
plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()

r2_values = []
max_error_values = []
neg_mean_absolute_error_values = []
neg_mean_squared_error_values = []
neg_root_mean_squared_error_values = []

for name, est in estimators:
    # Cross validation
    score = cross_validate(est, XX_train, yy_train, cv=5,
                           scoring=['r2', 'max_error', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_root_mean_squared_error'],
                           n_jobs=-1)

    print(score)
    # Saving the values
    r2_values.append(score['test_r2'])
    max_error_values.append(-score['test_max_error'])
    neg_mean_absolute_error_values.append(-score['test_neg_mean_absolute_error'])
    neg_mean_squared_error_values.append(-score['test_neg_mean_squared_error'])
    neg_root_mean_squared_error_values.append(-score['test_neg_root_mean_squared_error'])

# Plotting the values
fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(30, 5))

names = ['LR', 'DTR', 'SGD', 'GBR', 'KNN', 'RFR']
labels = names[:len(r2_values)]  # Use only the required number of labels

axs[0].boxplot(r2_values, labels=labels)
axs[0].set_title('R2')
axs[1].boxplot(max_error_values, labels=labels)
axs[1].set_title('MAX')
axs[2].boxplot(neg_mean_absolute_error_values, labels=labels)
axs[2].set_title('MAE')
axs[3].boxplot(neg_mean_squared_error_values, labels=labels)
axs[3].set_title('MSE')
axs[4].boxplot(neg_root_mean_squared_error_values, labels=labels)
axs[4].set_title('RMSE')

plt.suptitle('Cross-validation')
plt.show()

regression = 4  # Adjust the value of regression as needed

if regression < len(r2_values):
    print(u'R²: {:.3f} \u00B1 {:.3f}'.format(np.mean(r2_values[regression]), np.std(r2_values[regression])))
    print(u'MAX: {:,.0f} \u00B1 {:,.0f}'.format(np.mean(max_error_values[regression]), np.std(max_error_values[regression])))
    print(u'MAE: {:,.0f} \u00B1 {:,.0f}'.format(np.mean(neg_mean_absolute_error_values[regression]), np.std(neg_mean_absolute_error_values[regression])))
    print(u'MSE: {:,.0f} \u00B1 {:,.0f}'.format(np.mean(neg_mean_squared_error_values[regression]), np.std(neg_mean_squared_error_values[regression])))
    print(u'RMSE: {:,.0f} \u00B1 {:,.0f}'.format(np.mean(neg_root_mean_squared_error_values[regression]), np.std(neg_root_mean_squared_error_values[regression])))
else:
    print("Regression index is out of range, try lower. ")

"""# Saving the model"""

import pickle
from google.colab import files

# Save the trained model
with open('random_forest_model.pkl', 'wb') as file:
    pickle.dump(random_forest_model, file)

# Download the saved model file
files.download('random_forest_model.pkl')